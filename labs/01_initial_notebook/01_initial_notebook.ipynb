{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# MLOps workshop with Amazon SageMaker\n",
    "\n",
    "In this workshop we will demonstrate a journey to cloud-native machine learning starting from a more traditional approach to model development and training directly in Jupyter notebooks, to a managed data transformations and training with Amazon SageMaker. Once we will learn the concepts of cloud base processing and training, we will then build a fully automated pipelines with SageMaker Pipelines.\n",
    "\n",
    "## Module 01: Transform the data and train a model inside a Jupyter notebook.\n",
    "\n",
    "In this first notebook we will predict house prices based on the well-known [California housing dataset](http://lib.stat.cmu.edu/datasets/) with a simple regression model in Tensorflow 2. This public dataset contains 9 features regarding housing stock of towns in California area. Features include: average number of rooms, accessibility to radial highways, adjacency to a major river, etc.  \n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data.  We'll also set up a SageMaker Session to perform various operations, and specify an Amazon S3 bucket to hold input data and output.  The default bucket used here is created by SageMaker if it doesn't already exist, and named in accordance with the AWS account ID and AWS Region.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "batch_dir = os.path.join(os.getcwd(), 'data/batch')\n",
    "os.makedirs(batch_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "According to The [State of Data Science 2020](https://www.anaconda.com/state-of-data-science-2020) survey, data management, exploratory data analysis (EDA), feature selection, and feature engineering accounts for more than 66% of a data scientistâ€™s time.\n",
    "\n",
    "Exploratory Data Analysis is an approach in analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.\n",
    "EDA assists Data science professionals in various ways:\n",
    "\n",
    "- Getting a better understanding of data.\n",
    "- Identifying various data patterns.\n",
    "- Getting a better understanding of the problem statement.\n",
    "\n",
    "Numerical EDA gives you some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. \n",
    "Visual EDA on the other hand will give you insight into features and target relationship and distribution.\n",
    "\n",
    "First we'll load the California Housing dataset and explore the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download California Housing dataset\n",
    "\n",
    "For this workshop, we will use the California housing dataset.\n",
    "\n",
    "More info on the dataset:\n",
    "\n",
    "- This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/\n",
    "- The target variable is the median house value for California districts.\n",
    "- This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -zxf cal_housing.tgz 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "]\n",
    "df = pd.read_csv(\"CaliforniaHousing/cal_housing.data\", names=columns, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical EDA\n",
    "\n",
    "Check how big is dataset, how many and of what type features it has, and what is target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 attributes in each case of the dataset. They are:\n",
    "\n",
    "1. longitude - block group longitude\n",
    "2. latitude - block group latitude\n",
    "3. housingMedianAge - median house age in block group\n",
    "4. totalRooms - average number of rooms per household\n",
    "5. totalBedrooms - average number of bedrooms per household\n",
    "6. population - block group population\n",
    "7. households - average number of household members\n",
    "8. medianIncome - median income in block group\n",
    "9. medianHouseValue - median value of owner-occupied homes "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's summarize the data to see the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `df.describe()` We can see that the housing median age is around 28.639 years, looking at the mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze median house age in block group\n",
    "\n",
    "We will use `df.value_counts` that will return the unique rows for the feature `housingMedianAge`. This will help us to see how the houses age is distributed in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.value_counts(\"housingMedianAge\", sort=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin exploring the data.\n",
    "We will see how each feature is distributed in the dataset, in bins of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on `medianHouseValue` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "\n",
    "df['medianHouseValue'].hist(bins=100)\n",
    "plt.xlabel(\"Median House Value\", fontsize=14)\n",
    "plt.ylabel(\"Houses\", fontsize=13)\n",
    "plt.xticks(rotation=0)\n",
    "plt.title(\"Median House Value across the state of California (CA)\", fontsize=15)\n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a significant outlier here. Consider exploring it more. We will stop with EDA for now for the sake of time needed for the rest of the workshop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dataset transformation\n",
    "\n",
    "Next, we'll transform the dataset. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances. \n",
    "\n",
    "We'll now save the raw feature data, and also save the labels for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[['longitude','latitude','housingMedianAge','totalRooms','totalBedrooms','population','households','medianIncome']]\n",
    "Y = df[['medianHouseValue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Features:\", list(X.columns))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Dataset Type:\", type(X))\n",
    "print(\"Label set shape:\", Y.shape)\n",
    "print(\"Label set Type:\", type(X))\n",
    "\n",
    "# We partition the dataset into 2/3 training and 1/3 test set.\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next, we'll execute the data preprocessing as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = np.load(os.path.join(raw_dir, 'x_train.npy'))\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_files = glob.glob('{}/raw/*.npy'.format(data_dir))\n",
    "print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "for file in input_files:\n",
    "    raw = np.load(file)\n",
    "    # only transform feature columns\n",
    "    if 'y_' not in file:\n",
    "        transformed = scaler.transform(raw)\n",
    "    if 'train' in file:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(train_dir, 'y_train.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('Saved labeled training data in {}\\n'.format(output_path))\n",
    "        else:\n",
    "            output_path = os.path.join(train_dir, 'x_train.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('Saved transformed training data in {}\\n'.format(output_path))\n",
    "    else:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(test_dir, 'y_test.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('Saved labeled test data in {}\\n'.format(output_path))\n",
    "        else:\n",
    "            output_path = os.path.join(test_dir, 'x_test.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('Saved transformed test data in {}\\n'.format(output_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#  Training\n",
    "\n",
    "Now that we've prepared a dataset, we can move on to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(8,))\n",
    "    hidden_1 = tf.keras.layers.Dense(8, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(4, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_train_data(train_dir)\n",
    "x_test, y_test = get_test_data(test_dir)\n",
    "\n",
    "device = '/cpu:0'\n",
    "print(device)\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "\n",
    "with tf.device(device):\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The unzipped archive should include the assets required by TensorFlow Serving to load the model and serve it, including a .pb file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.save('model' + '/1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -R model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Scoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('model/1')\n",
    "\n",
    "x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"\\nTest MSE :\", scores)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
